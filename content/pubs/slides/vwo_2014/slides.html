<!DOCTYPE html>
<html>
  <head>
    <title>How to change your opinion and make a decision</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);

      body {
        font-family: 'Droid Serif';
        font-size: 18px;
      }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      h1 { font-size: 4em; }
      h2 { font-size: 2em; }
      h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        -moz-border-radius: 5px;
        -web-border-radius: 5px;
        background: #e7e8e2;
        border-radius: 5px;
        font-size: 16px;
      }
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #777872;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 0.8em;
      }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      li {
        font-size: 18px;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 2em;
      }
      img {
        max-width: 800px;
      }
    </style>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-30538320-1']);
  _gaq.push(['_setDomainName', 'chrisstucchio.com']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
  </head>
  <body>
    <textarea id="source">
name: inverse
class: center, middle, inverse

# How to change your opinion and make a decision
### From a bayesian point of view
Chris Stucchio, [BayesianWitch](http://www.bayesianwitch.com)

---
# A decision to make

## Designer's Choice

![advertise on patch](patch_advertise_green.png)

---
# A decision to make

## Sales Guy Choice

![advertise on patch](patch_advertise_red.png)

---
# A decision to make

## Analytical Marketer: "Don't forget to try pink and purple"

![advertise on patch](patch_advertise_purple.png)

---
# A decision to make

## Analytical Marketer: "Don't forget to try pink and purple"

![advertise on patch](patch_advertise_pink.png)

---
name: inverse
class: center, middle, inverse

# Frequentist Statistics
## are really weird
## (But for some reason, everyone still insists on doing them.)
---
# Frequentist Statistics
## P-values
- Run an experiment.
- Compute a p-value.
- If p-value < 0.05, declare victory.
- Probability of observing data given that null hypothesis is true.

## Confidence intervals
- Set of possible null hypothesis that would not be rejected by the data.
- Equivalently: In a sequence of repeated experiments, is likely to contain the true value.

---
# P-values
You compare the means of your control and experimental groups (say 20 subjects in each sample). Further, suppose you use a simple independent means t-test and your result is (t = 2.7, d.f. = 18, p = 0.01). Please mark each of the statements below as “true” or “false”.

1. You have absolutely disproved the null hypothesis (that is, there is no difference between the population means).
2. You have found the probability of the null hypothesis being true.
3. You have absolutely proved your experimental hypothesis (that there is a difference between the population means).
4. You can deduce the probability of the experimental hypothesis being true.
5. You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision.
6. You have a reliable experimental finding in the sense that if, hypothetically, the experiment were repeated a great number of times, you would obtain a significant result on 99% of occasions.

---
# Confidence intervals
An experiment was run. The 95 % confidence interval for the mean ranges from 0.1 to 0.4. Which of the following statements is true?

1. The probability that the true mean is greater than 0 is at
least 95 %.
2. The probability that the true mean equals 0 is smaller than
5 %.
3. The “null hypothesis” that the true mean equals 0 is likely
to be incorrect.
4. There is a 95 % probability that the true mean lies between
0.1 and 0.4.
5. We can be 95 % confident that the true mean lies between
0.1 and 0.4.
6. If we were to repeat the experiment over and over, then
95 % of the time the true mean falls between 0.1 and 0.4.
---
# P-values
You compare the means of your control and experimental groups (say 20 subjects in each sample). Further, suppose you use a simple independent means t-test and your result is (t = 2.7, d.f. = 18, p = 0.01). Please mark each of the statements below as “true” or “false”.

1. You have absolutely disproved the null hypothesis (that is, there is no difference between the population means).
2. You have found the probability of the null hypothesis being true.
3. You have absolutely proved your experimental hypothesis (that there is a difference between the population means).
4. You can deduce the probability of the experimental hypothesis being true.
5. You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision.
6. You have a reliable experimental finding in the sense that if, hypothetically, the experiment were repeated a great number of times, you would obtain a significant result on 99% of occasions.

# All are false!
---
# Confidence intervals
An experiment was run. The 95 % confidence interval for the mean ranges from 0.1 to 0.4. Which of the following statements is true?

1. The probability that the true mean is greater than 0 is at
least 95 %.
2. The probability that the true mean equals 0 is smaller than
5 %.
3. The “null hypothesis” that the true mean equals 0 is likely
to be incorrect.
4. There is a 95 % probability that the true mean lies between
0.1 and 0.4.
5. We can be 95 % confident that the true mean lies between
0.1 and 0.4.
6. If we were to repeat the experiment over and over, then
95 % of the time the true mean falls between 0.1 and 0.4.

# All are false!
---
# P-values

80% of psychology professors teaching statistical methodology made at least 1 mistake. 97% of academic psychologists and 100% of psychology students made one mistake.

Most common answers are (4) and (5).

The article notes that many *statistics textbooks* make at least one of these mistakes.

Source: [Misinterpretations of Significance: A Problem Students Share with Their Teachers?](http://myweb.brooklyn.liu.edu/cortiz/PDF%20Files/Misinterpretations%20of%20Significance.pdf), *by Heiko Haller & Stefan Krauss. Methods of Psychological Research Online 2002, Vol.7, No.1*
---
# Confidence intervals

These questions were shown to psychology 1'st years, masters and researchers.

<table>
<tr> <th>Statement</th> <th>First year</th> <th>Masters</th>  <th>Researcher</th>  </tr>
<tr><td>1</td><td>51%</td><td>32%</td><td>38%</td></tr>
<tr><td>2</td><td>55%</td><td>44%</td><td>47%</td></tr>
<tr><td>3</td><td>73%</td><td>68%</td><td>86%</td></tr>
<tr><td>4</td><td>58%</td><td>50%</td><td>59%</td></tr>
<tr><td>5</td><td>49%</td><td>50%</td><td>55%</td></tr>
<tr><td>6</td><td>66%</td><td>79%</td><td>58%</td></tr>
</table>

3% of researchers thought 0 answers were true and 9% said only 1 answer was true. 47% thought 3 or 4 of these statements were true!

If only 3% of Ph.D.'s doing statistical research can understand frequentist statistics, what hope does an e-commerce marketer have?

Source: [Robust misinterpretation of confidence intervals](http://www.ejwagenmakers.com/inpress/HoekstraEtAlPBR.pdf), *Rink Hoekstra & Richard D. Morey & Jeffrey N. Rouder & Eric-Jan Wagenmakers*
*Psychon Bull Rev 14 Jan 2014*

---
class: center, middle

# If we used Bayesian methods, all answers besides (6) would be correct.

(Answer (6) is syntactically very similar to a correct definition, and students/professors are likely misremembering that.)

---
class: center, middle, inverse
# Bayesian statistics
---
name: inverse
class: center, middle, inverse
# Conversion Rate

Conversion rate is the probability of a **visitor** converting to a **customer**.

    Click here to learn ONE WEIRD TRICK
    for becoming taller!

    http://www.growtaller.com


[Details on this can be found on my blog](http://www.chrisstucchio.com/blog/2013/bayesian_analysis_conversion_rates.html)
---

# Conversion Rate

"True" conversion rate:

    ctr = P( conversion | visitor )

Empirical conversion rate:

    # of conversions / # of visitors

## Not the same thing

Relation between empirical and true conversion rate:

    P( # conversions = C | # visitors) = Const * ctr^(C)(1-ctr)^(# visitors - C)


---

# Example: A coin

"True" conversion rate:

    ctr = P( conversion | visitor ) = 0.5

Empirical conversion rate:

    6 heads / 10 clicks = 0.6

## Not the same thing

Approximately, but not identically equal.

---

# True conversion rate

Can never be known exactly. But we can estimate it based on:

## Prior Knowledge

A 90% conversion rate is crazy because 30% of people are already tall enough. The most we'll ever get is 70%.

## Data

I showed the ad to 1024 people and 37 clicked it. Pretty sure the conversion rate is far less than 70%.

---
# Probability distributions

In code:

    In [20]: ctr
    Out[20]:
    array([ 0.  ,  0.01,  0.02,  0.03,  0.04,  0.05,  0.06,  0.07,  0.08,
            0.09,  0.1 ,  0.11,  0.12,  0.13,  0.14,  0.15,  0.16,  0.17,
            ...
            0.81,  0.82,  0.83,  0.84,  0.85,  0.86,  0.87,  0.88,  0.89,
            0.9 ,  0.91,  0.92,  0.93,  0.94,  0.95,  0.96,  0.97,  0.98,  0.99])

    In [33]: pctr
    Out[33]:
    array([  1.81296534e-01,   1.49781508e-01,   1.23505113e-01,
             1.01637144e-01,   8.34724053e-02,   6.84129001e-02,
             ...
             4.98344117e-28,   2.10713975e-30,   9.50515970e-34,
             1.81296534e-39])

Important fact:

    In [29]: pctr.sum()
    Out[29]: 1.0000000000000004

---
# Probability distributions

## Interpretation

`pctr[0]` represents the probability that the true conversion rate is 0%.

`pctr[1]` represents the probability that the true conversion rate is 1%.

etc.

---
# Probability distributions

## What is probability that CTR is between 10% and 30%?

    In [35]: pctr[where(logical_and(ctr >= 0.1, ctr =< 0.3))].sum()
    Out[35]: 0.12225830312891443

---

# Probability distributions

![test](example_pdf_1.png)

ctr is unknown, could be anything

---
# Probability distributions

![test](example_pdf_2.png)

ctr is approximately 20%, could be between 5% and 45%.

---
# Probability distributions

![test](example_pdf_3.png)

ctr is between 50% and 70%.

---
class: center, middle

![your opinon](your_opinion.png)

---
# In my uneducated opinion

## Stating your prior

![prior](prior.png)

What it means: I think a low CTR is far more likely than a high one.

---
class: center, middle
# "...when events change, I change my mind. What do you do?"
## Paul Samuelson

(Often incorrectly attributed to Keynes.)

---
# Changing your opinion



In Bayesian Statistics, one's opinion changes based on Bayes Rule.

                      P(evidence|ctr) P(ctr)
    P(ctr|evidence) = ----------------------
                           P(evidence)

Given an old opinion `P(ctr)`, you have a clear rule for changing your opinion.

---
# Changing your opinion

Evidence: Showed the advertisement to a user and they clicked

    P(evidence|ctr) = ctr

Doing some math:

                      ctr * P(ctr)
    P(ctr|evidence) = ------------
                       P(evidence)

In code:

    def update_belief_from_click(pctr):
        unnormalized = ctr * pctr
        return unnormalized / unnormalized.sum()

---
# Changing your opinion

What we believe before evidence:

![prior](prior.png)

---
# Changing your opinion

What we believe after seeing evidence:

![prior](posterior_one_click.png)

---
# Changing your opinion

Evidence: Showed the advertisement to a user and they did NOT click

    P(evidence|ctr) = 1 - ctr

Doing some math:

                      (1 - ctr) * P(ctr)
    P(ctr|evidence) = ------------
                       P(evidence)

In code:

    def update_belief_from_no_click(pctr):
        unnormalized = (1-ctr) * pctr
        return unnormalized / unnormalized.sum()

---
# Changing your opinion

What we believe after seeing one click:

![prior](posterior_one_click.png)

---
# Changing your opinion

What we believe after seeing one click, but 4 displays with no clicks:

![prior](posterior_one_click_4_noclick.png)

---
# Changing your opinion

What we believe after seeing one click, but 20 displays with no clicks:

![prior](posterior_one_click_20_noclick.png)

---
# Changing your opinion

What we believe after seeing 4 clicks and 203 displays with no clicks:

![prior](posterior_lots_of_displays.png)

---
# Disagreement

Paras and Sparsh might have different opinions before gathering data.

![prior](two_opinions_prior.png)

---
# Disagreement

After we gather enough data, their opinions will converge.

![prior](two_opinions_posterior.png)

(This assumes they are both rational Bayesians attempting to maximize profit. In dysfunctional organizations this is not true.)

---
class: center, middle
# Opinions and Actions

[More details on the BayesianWitch blog](http://www.bayesianwitch.com/blog/2014/bayesian_ab_test.html)
---
# How do we make a decision?

## Difficulties

- Non-zero probability of making mistake
- Must stop test, if only to delete code
- How long must we run it for?
- Can we stop the test early?

## Possible Goals

- Avoid mistakes
- Maximize profit
- Convince our boss we made a profit

*These are all different!*
---
# Examples

## Maximize profit

Run a test until the result is barely significant.

**What we know:** (After 1000 samples) Version B is not worse than A, and might be better. **Ship B!**

Employed by: Quant traders in the shares markets, agile startups.

## Convince our boss we made a profit

Run a test until the result has high confidence.

**What we know:** (after 5000 samples) We are confident that B is better by at least 20%, and we probably should have shipped it 4000 customers ago.

Employed by: The (American) Food and Drug Administration (FDA), bureaucratic risk-averse corporations.

---
# Loss functions

    def loss(world_state, choice_made):
        ....

The value `loss( X, B)` is how much you would lose if the world looks like `X` and you made the choice `B`.

## Expected loss:

```
def expected_loss(loss, posterior, choice):
    return sum( loss(world_states, choice) * posterior )
```

This represents the *average* losses you believe will happen if you make a given choice.
---
# Choice of loss function

```
def error_probability(world_state, choice):
    conv_rate_a, conv_rate_b = world_state
    if choice == A:
        return conv_rate_a >= conv_rate_b
    if choice == B:
        return conv_rate_b >= conv_rate_a
```
- In this case, `expected_loss` is the same as `P(choice is correct)`.
- Treats all mistakes as equal, i.e. choosing A when `world_state=(0.10, 0.101)` is just as bad as choosing A when `world_state=(0.10, 0.20)`.
---
# Choice of loss function

```
def loss_of_conversions(world_state, choice):
    conv_rate_a, conv_rate_b = world_state
    if choice == A:
        return max(conv_rate_b - conv_rate_a, 0)
    if choice == B:
        return max(conv_rate_a - conv_rate_b, 0)
```
- The cost of a mistake is the difference in conversion rates. Choosing A when `world_state=(0.10, 0.101)` has cost `0.001`, while choosing A when `world_state=(0.10, 0.20)` has cost `0.10`.
- May result in making a choice, even when `P(choice is wrong)` is high. **Analogy:** I don't really know which pani puri cart tastes better, but I'm only out 20 rupees if I screw up - *just pick one and move on with your life*.
- **This is the loss function I recommended.**
---
# Choice of loss function

```
def risk_averse(x):
    if x > 0:
        return max(x, 0.01)
    else:
        return 0

def loss_of_conversions(world_state, choice):
    conv_rate_a, conv_rate_b = world_state
    if choice == A:
        return max(risk_averse(conv_rate_b - conv_rate_a), 0)
    if choice == B:
        return max(risk_averse(conv_rate_a - conv_rate_b), 0)
```
- If I make a mistake I look bad to my boss, even if the harm is small.
- So round small errors up.

---
# Choice of loss function
![advertise on patch](loss_functions.png)

---
# Important parameters

## Prior

Already talked about this.

## Threshold of caring

Suppose version A converts at 20% and version B converts at 10%. You definitely care.

Suppose A converts at 10% and B converts at 10.01%. You probably don't care.

    threshold_of_caring = if lift gained/lost is smaller, I don't care

---
# A/B testing

Some data:

<table>
<tr><th>Version</th><th>Displays</th><th>Conversions</th></tr>
<tr><td>A</td><td>581</td><td>112</td></tr>
<tr><td>B</td><td>583</td><td>75</td></tr>
</table>

Now what do we do? Stop the test and choose B? Continue the test?

## What if we made a mistake?

---
# Joint Probability Distribution

Ran test for A and B individually. Computed posteriors.

![ab test posteriors](ab_test_posteriors.png)

---
# Joint Probability Distribution

![ab test posteriors](ab_test_joint_probability_distribution.png)

Point in plane is `(ctrA, ctrB)`. Color is probability density.

---
# Consequences of an error

```
def expected_loss(loss, posterior, choice):
    return sum( loss(world_states, choice) * posterior )
```
Sum is taken over all values of `(ctrA, ctrB)` for which `ctrA > ctrB` (above yellow line). The function `loss` is a specific choice of loss function.

## Assume we choose A

```
Loss = expected_loss( loss, posterior, A)
```

## Assume we choose B

```
Loss = expected_loss( loss, posterior, B)
```

---
# Stopping condition

```
valid_choices = [ c for c in choices
                    if expected_loss(loss, posterior, c) < threshold_of_caring ]

if len(valid_choices) > 0:
    stop_test_choose_from(valid_choices)
else:
    continue_test()
```

Stop test when the amount of lift we expect to lose **assuming** we make the wrong choice is so low that we don't care. Otherwise continue.

## The resulting choice may not be better

If B is definitely not worse than A, but might be better, then it makes sense to choose B. It's a bet with no downside and possible upside. Give up scientific truth for faster decisions.

**Analogy:** Flip a coin. If it comes up heads I give you 100rs. If it comes up tails, you pay me 1 paise. A good bet?
---
# Don't look for truth

![ab test posteriors](certain_choice_uncertain_result.png)

B could be anywhere from 0% to 20% better (in terms of absolute lift, i.e. `ctr_b - ctr_a`), but we don't know which.

**Choose B** - tiny risk, large possible reward. Waiting for truth *will not increase profit*.
---
# What if the choice doesn't matter?

![ab test posteriors](loss_function_x_posterior.png)

Because posterior is fat (very little data), loss function for choice A (and B) is high.

**Fact:** For beta distribution `variance = O(1/N)`. Small number of samples => high variance.
---
# What if the choice doesn't matter?

![ab test posteriors](loss_function_x_posterior2.png)

Because posterior is thin, expected loss for choice A (and B) is low.

**Fact:** For beta distribution `variance = O(1/N)`. Large number of samples => low variance.
---
# What if the choice doesn't matter?
## Don't look for truth

Eventually the loss from making a choice will be arbitrarily low due solely to the posterior becoming narrow.

```
valid_choices = [A,B]
```

**Choose A** - tiny risk, tiny reward. Waiting for truth *will not increase profit*.

**Choose B** - tiny risk, tiny reward. Waiting for truth *will not increase profit*.

![coin flip](http://zmainvestments.com/wp-content/uploads/2011/11/coin-flip.jpg)

---
## Why history chose frequentist methods

![old timey computer](old_timey.jpg)

---
# What is probability of an error?

**NOT frequentist p-value!**

Rather (assuming choice A is made):

```
P(error) = sum( posterior[where(ctr[A] > ctr[B])] )
```

(Flip A and B if choice B was made.)

---
# With 95% probability, what is the CTR?

**NOT frequentist confidence interval!**

Rather compute a Bayesian *credible interval*. Find `a,b` so that:

```
sum(posterior[where(a < ctr && ctr < b)]) >= 0.95
```

![old timey computer](credible_interval.png)

```
P(a < ctr < b) = 0.95
```

---
# Pros/cons for Bayesian methods
## Benefits

- Lets us stop tests early.
- Or run them for extra time if we need to.
- Can incorporate a more detailed model than simply 1 conversion rate, if needed.
- End results are highly intuitive.

## Drawbacks

- Standard frequentist test takes a couple of microseconds and 2 doubles.
- Bayesian test requires up to 1 second and 1024x1024 array of doubles (16mb ram).

---
# Conclusions

## Use A/B testing for long term feature decisions
## Use Bandit Algorithms for Transient/high volume content
## Use Bayesian Statistics

    </textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create();
    </script>
  </body>
</html>