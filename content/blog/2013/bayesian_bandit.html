---
title: How the Bayesian Bandit works
created: !!timestamp '2013-04-13 21:00:00'
tags:
    - statistics
    - bayesian reasoning
    - bandit algorithms
---

{% mark excerpt -%}

Consider the problem - you have two possible titles for a news story - "Murder victim found in adult entertainment venue" and "Headless Body found in Topless Bar". A common approach is to run both versions and measure the click-through rate (CTR) for each story. At some point, when the measured CTR for one title exceeds that of the other title, you'll switch to the one with the highest for all users. I'm glossing over many details here, but algorithms for solving this problem are called *bandit algorithms*.

In this blog post I'll describe one of my favorite bandit algorithms, the Bayesian Bandit, and show why it is an excellent method to use for problems which give us more information than typical bandit algorithms.

{%- endmark %}

## The problem to be solved, and the underlying model

Ultimately the problem we want to solve is the following. Consider an article being published on a website. The author or editor has come up with several possible titles - "Murder victim found in adult entertainment venue", "Headless Body found in Topless Bar", etc. We want to choose the title with the best click through rate (CTR). Let us represent each CTR by $\theta_i$ - i.e., $\theta_i$ is the probability that an individual user will click on the i-th title. As a simplifying assumption, we assume that these rates $\theta_i$ do not change over time.

The goal of the bandit algorithm is to do the following. To begin with, it should display both titles to a random selection of users. It will also measure which titles are clicked on more. Over time, it will use these observations to infer which articles have the higher CTR. Then, once the estimation of the CTR becomes more precise, it will preferentially display articles with the higher CTR.

The ultimate goal here is simply to maximize the CTR rate.

## The Bayesian Approach

In the model described above, we have $N$ possible story titles, each of which has a click through rate $\theta_i$. Unfortunately we do not know what $\theta_i$ is. Since we are following a Bayesian approach, we will construct a *probability distribution* which represents our *belief* about what the actual value of $\theta_i$ is. Although there is a real $\theta_i$ out there somewhere, we simply don't know what it is.

![](bayesian_bandit/beliefs_about_theta.png)

In the figure above, we believe that $\theta_i$ is somewhere between 0.1 and 0.7, with values of 0.3-0.4 being considerably more likely than values of 0.1-0.2 or 0.6-0.7. For those who forgot STATS 101, the area under this curve between the points a and b is the probability thta $\theta_i$ lies between a and b. I.e.:

$$ P(\theta_i < \theta_i < b) = \int_a pdf(x) dx $$

The basic idea behind Bayesian methods is to update our beliefs based on evidence. As we gather more data by showing different titles to other users and observing click throughs, we can incrementally narrow the width of the probability distribution.

As in all Bayesian inference, we need to choose a *prior*. The prior is something we believe to be true *before we have any evidence* - i.e., before we have shown the title to any visitors. This is just a starting point - after enough evidence is gathered, our prior will play a very minimal role in what we actually believe. But choosing a good prior is important both for mathematical simplicity, and because if your prior is accurate, you don't need as much evidence to get the correct answer.

For mathematical simplicity, we will choose the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) because that will give us a [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior).

$$ P(\theta_i=x) = \frac{ x^{\alpha_i-1}(1-x)^{\beta_i-1} }{B(\alpha_i, \beta_i)} \equiv f_{\alpha_i,\beta_i}(x) $$

The parameters $\alpha_i, \beta_i > 1$ are the prior parameters. One reasonable choice is $\alpha_i=\beta_i=1$, which amounts to the uniform distribution on $[0,1]$. What this means is that we are assuming that all possible values of $\theta_i$ are equally likely.

### Updating our beliefs

Now we address the question of using evidence.

After showing title $i$ to $n_i$ visitors, we have observed that $s_i$ of them have actually clicked on the title. We now want to compute the *posterior* distribution, which is to say the distribution that represents our beliefs *after* we have evidence.

This is done with Bayes rule:

$$ P(\theta_i = x | n_i, s_i) = \frac{ P(n_i, s_i | \theta_i=x) P(\theta_i=x) }{ P(n_i, s_i)} $$

(Readers who dislike math should jump ahead to the graph.)

Now, because I chose the beta distribution as the prior, I can avoid doing a lot of algebraic work.  The rough idea here is the following. Ignoring constants (numbers which do not vary with $x$), we know that $P(n_i, s_i | \theta_i=x) = x^{s_i}(1-x)^{n_i-s_i}$ - this is the binomial distribution. Substituting this as well as the definition of the prior in, we find that:

$$ P(\theta_i = x | n_i, s_i) = C x^{s_i}(1-x)^{n_i-s_i} x^{\alpha_i-1} (1-x)^{\beta_i-1} = C x^{\alpha_i+s_i-1} (1-x)^{\beta_i+n_i-s_i-1} = f_{\alpha_i+s_i, \beta_i+n_i-s_i}(x) $$

I'm glossing over the details showing that $C = 1/B(\alpha_i+s_i, \beta_i+n_i-s_i)$ because they are not very interesting.

The key idea here is that to update our probability distribution describing $\theta_i$, we need only update the parameters of our beta distribution.

So what does this mean in practice? As we run more experiments, our probability distribution on where $\theta_i$ lives becomes sharper:

![](bayesian_bandit/beta_distribution_evolution.png)

Before we run any experiments, $\theta_i$ could be anything (as represented by the blue line). Once we have run 700 experiments, yielding 175 click throughs, we are reasonably confident that $\theta_i$ lives roughly between 0.2 and 0.3.

So what we've done so far is figured out how to estimate what our click through rates actually are based on empirical evidence. But that doesn't actually give us a method of optimizing them yet.

## Optimizing click throughs

Now that we have a method of representing our beliefs about CTRs, it is useful to construct an algorithm to identify the best ones. There are many popular choices - I've written about the [UCB Algorithm](/blog/2012/bandit_algorithms_vs_ab.html) before, and I consider it a good choice.

But my new favorite method is a [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method) method which I'll describe now.

The ultimate goal of the bandit algorithm is to display to the user whichever title has the highest CTR. One method of estimating the CTRs of the articles is to *sample* the posterior distribution. I.e., suppose we have two possible titles, from which we have drawn $n_0=200, s_0=64$ and $n_1=180, n_2=40$. Then one possible set of samples we might observe is this:

![](bayesian_bandit/beta_distribution_sampling1.png)

For title 0, our sample of $\theta_0$ has worked out to be 0.35, while our sample of $\theta_1$ is only 0.28. Since $\theta_0 = 0.35 > \theta_1 = 0.28$, we will display title 0 to the user.

However, there was no guarantee that things worked out this way. It was possible, although less likely, that $\theta_1$ could come out larger than $\theta_0$:

![](bayesian_bandit/beta_distribution_sampling2.png)

In this case, we would have displayed title 1 to the user rather than title 0.

The net result is that for overlapping probability distributions, we will display the title with the larger *expected* CTR the majority of the time. But occasionally, we will draw from the other distributions simply because it is within the realm of possibility that they are greater.

As we gather more data our probability distributions will become narrower and a clear winner will become apparent. When this occurs, we will almost surely choose the winner:

![](bayesian_bandit/beta_distribution_sampling3.png)

In python, the algorithm looks like this:

<script src="https://gist.github.com/stucchio/5383149.js"></script>

The results of this algorithm are exactly what any good bandit algorithm should do. I ran the following simulation, giving the beta bandit two titles - title 0 had a CTR of 0.25, title 1 had a CTR of 0.35. To start with, both titles were displayed to the user with roughly equal probability. Over time, evidence accumulated that title 1 was considerably better than title 0. At this point the algorithm switched to displaying primarily title 1, and the overall CTR of the experiment converged to 0.35 (the optimal CTR).

![](bayesian_bandit/beta_bandit_results.png)

Source code to generate this graph is available [here](https://gist.github.com/stucchio/5383015#file-beta_bandit_test-py). This method is called [Thompson Sampling](https://en.wikipedia.org/wiki/Thompson_sampling) and is a a fairly popular method in Bayesian AI techniques.

## Modularity

The key business advantage that the Bayesian Bandit has over other bandit algorithms is not that it converges more quickly than other bandit algorithms (at least in the standard case). Rather, the key advantage of the Bayesian Bandit is that it is modular. I'll illustrate with an example.

Lets go back to our running example of title selection. We have two possible titles to consider - "Murder victim found in adult entertainment venue" and "Headless Body found in Topless Bar". Even without any evidence, we can speculate that the catchy title will drive more clicks. Suppose further that we actually did a study - we had editors rate a collection of titles as "catchy" or "not catchy". We then measured the CTR for both groups, and discovered that in aggregate, "catchy" titles had a better CTR. I.e., our empirical data looks something like this:

![](bayesian_bandit/empirical_prior.png)

This is important data, and it would be great to incorporate it into our bandit algorithm. For many methods (e.g., [UCB](/blog/2012/bandit_algorithms_vs_ab.html)) it's somewhat difficult to do this - there is no obvious parameter to tune as a result of our prior empirical data. The beauty of a Bayesian method is that it gives you a clear and meaningful place to plug this information in, namely the prior.

The first step is to fit a theoretical distribution to the empirical data. Due to the fact that I chose the empirical data to be very nice, a beta distribution fits well [1] - specifically beta distributions with $(\alpha_0, \beta_0) = (9,20)$ and $(\alpha_1, \beta_1) = (4,20)$.

![](bayesian_bandit/theoretical_prior.png)

Then the only modification needed to the algorithm is to plug these variables into the prior:

$$ P(\theta_0=x) = \frac{ x^{9-1}(1-x)^{20-1} }{B(9, 20)} $$

$$ P(\theta_1=x) = \frac{ x^{4-1}(1-x)^{20-1} }{B(4, 20)} $$

Everything else remains unchanged.

The key benefit here is that a bandit algorithm takes some time to learn which distribution is the best. By incorporating prior knowledge, we can give it a head start.

### Empirics of including priors

//finish this bit

## Conclusion

Bandit algorithms are pretty awesome, and incorporating priors makes them even more awesome. Bayesian bandit algorithms let you incorporate priors. //This should suck less

[1] If a single Beta distribution doesn't fit, one can also use a convex combination of Beta distributions. The math works out just as nicely.
